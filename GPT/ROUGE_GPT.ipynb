{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b7a93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: six in /home/lavanya/lavanya/lib/python3.7/site-packages (from Rouge) (1.16.0)\n",
      "Installing collected packages: Rouge\n",
      "Successfully installed Rouge-1.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3db86521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/lavanya/lavanya/lib/python3.7/site-packages (23.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-23.2-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.1.2\n",
      "    Uninstalling pip-23.1.2:\n",
      "      Successfully uninstalled pip-23.1.2\n",
      "Successfully installed pip-23.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e651f8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/lavanya/lavanya/lib/python3.7/site-packages (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2d8aa65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Scores: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1630/1630 [01:32<00:00, 17.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Overlap Score: 0.5876360810826065\n",
      "Average Coherence Score: 0.37743062563826585\n",
      "Average Informativeness Score: 0.570288317727583\n"
     ]
    }
   ],
   "source": [
    "#### Combined summary Rogue Score\n",
    "from rouge import Rouge\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# Load the DataFrame from the Excel file\n",
    "input_file = \"GPT3.5_All_Summaries_Mergedd.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Create a Rouge object\n",
    "rouge = Rouge()\n",
    "\n",
    "# Define lists to store the overlap, coherence, and informativeness scores\n",
    "overlap_scores = []\n",
    "coherence_scores = []\n",
    "informativeness_scores = []\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Iterate over the rows in the DataFrame with a progress bar\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
    "    # Get the original text and the generated summary\n",
    "    abstract = row['Abstract Summary']\n",
    "    claim = row['Claims Summary']\n",
    "\n",
    "    # Clean the abstract and claims text\n",
    "    abstract = re.sub(r'[^\\x00-\\x7F]+', '', str(abstract))\n",
    "    claims = re.sub(r'[^\\x00-\\x7F]+', '', str(claim))\n",
    "\n",
    "    # Combine the cleaned abstract and claims\n",
    "    original_text = abstract+' '+claims\n",
    "    generated_summary = str(row['Summary(Abstract Summary+ Claim Summary)'])\n",
    "\n",
    "    # Calculate ROUGE scores\n",
    "    scores = rouge.get_scores(generated_summary, original_text)\n",
    "\n",
    "    # Extract the relevant scores\n",
    "    overlap_score = scores[0]['rouge-1']['f']\n",
    "    coherence_score = scores[0]['rouge-2']['f']\n",
    "    informativeness_score = scores[0]['rouge-l']['f']\n",
    "\n",
    "    # Append the scores to the respective lists\n",
    "    overlap_scores.append(overlap_score)\n",
    "    coherence_scores.append(coherence_score)\n",
    "    informativeness_scores.append(informativeness_score)\n",
    "\n",
    "# Add the scores to the DataFrame\n",
    "df[\"Overlap_Score['rouge-1']\"] = overlap_scores\n",
    "df[\"Coherence_Score['rouge-2']\"] = coherence_scores\n",
    "df[\"Informativeness_Score['rouge-l']\"] = informativeness_scores\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "output_file = \"Summary(Abstract Summary+ Claim Summary).xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "# Print the average scores\n",
    "print(\"Average Overlap Score:\", sum(overlap_scores) / len(overlap_scores))\n",
    "print(\"Average Coherence Score:\", sum(coherence_scores) / len(coherence_scores))\n",
    "print(\"Average Informativeness Score:\", sum(informativeness_scores) / len(informativeness_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a534081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Scores: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1630/1630 [00:04<00:00, 355.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Overlap Score: 0.17802591832610024\n",
      "Average Coherence Score: 0.06477827013121165\n",
      "Average Informativeness Score: 0.16508555006301706\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### Claims Rogue Score\n",
    "from rouge import Rouge\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# Load the DataFrame from the Excel file\n",
    "input_file = \"GPT3.5_All_Summaries_Mergedd.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Create a Rouge object\n",
    "rouge = Rouge()\n",
    "\n",
    "# Define lists to store the overlap, coherence, and informativeness scores\n",
    "overlap_scores = []\n",
    "coherence_scores = []\n",
    "informativeness_scores = []\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Iterate over the rows in the DataFrame with a progress bar\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
    "    # Get the original text and the generated summary\n",
    "    claim = row['Summary(Abstract Summary+ Claim Summary)']\n",
    "\n",
    "    # Clean the abstract and claims text\n",
    "    claims = re.sub(r'[^\\x00-\\x7F]+', '', str(claim))\n",
    "\n",
    "    # Combine the cleaned abstract and claims\n",
    "    original_text = claims\n",
    "    generated_summary = str(row['Summary of Summary(Abstract Summary+ Claim Summary)'])\n",
    "\n",
    "    # Calculate ROUGE scores\n",
    "    scores = rouge.get_scores(generated_summary, original_text)\n",
    "\n",
    "    # Extract the relevant scores\n",
    "    overlap_score = scores[0]['rouge-1']['f']\n",
    "    coherence_score = scores[0]['rouge-2']['f']\n",
    "    informativeness_score = scores[0]['rouge-l']['f']\n",
    "\n",
    "    # Append the scores to the respective lists\n",
    "    overlap_scores.append(overlap_score)\n",
    "    coherence_scores.append(coherence_score)\n",
    "    informativeness_scores.append(informativeness_score)\n",
    "\n",
    "# Add the scores to the DataFrame\n",
    "df[\"Overlap_Score['rouge-1']\"] = overlap_scores\n",
    "df[\"Coherence_Score['rouge-2']\"] = coherence_scores\n",
    "df[\"Informativeness_Score['rouge-l']\"] = informativeness_scores\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "output_file = \"Summary of Summary(Abstract Summary+ Claim Summary).xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "# Print the average scores\n",
    "print(\"Average Overlap Score:\", sum(overlap_scores) / len(overlap_scores))\n",
    "print(\"Average Coherence Score:\", sum(coherence_scores) / len(coherence_scores))\n",
    "print(\"Average Informativeness Score:\", sum(informativeness_scores) / len(informativeness_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1d41fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Scores: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1630/1630 [04:35<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Overlap Score: 0.4238528161790118\n",
      "Average Coherence Score: 0.2169951596854535\n",
      "Average Informativeness Score: 0.40531427846956597\n"
     ]
    }
   ],
   "source": [
    "#### Combined summary Rogue Score\n",
    "from rouge import Rouge\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# Load the DataFrame from the Excel file\n",
    "input_file = \"GPT3.5_All_Summaries_Mergedd.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Create a Rouge object\n",
    "rouge = Rouge()\n",
    "\n",
    "# Define lists to store the overlap, coherence, and informativeness scores\n",
    "overlap_scores = []\n",
    "coherence_scores = []\n",
    "informativeness_scores = []\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Iterate over the rows in the DataFrame with a progress bar\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
    "    # Get the original text and the generated summary\n",
    "    abstract = row['Abstract']\n",
    "    claim = row['Claims']\n",
    "\n",
    "    # Clean the abstract and claims text\n",
    "    abstract = re.sub(r'[^\\x00-\\x7F]+', '', str(abstract))\n",
    "    claims = re.sub(r'[^\\x00-\\x7F]+', '', str(claim))\n",
    "\n",
    "    # Combine the cleaned abstract and claims\n",
    "    original_text = abstract+' '+claims\n",
    "    generated_summary = str(row['Summary Of (Abstrct+Claim) As single input'])\n",
    "\n",
    "    # Calculate ROUGE scores\n",
    "    scores = rouge.get_scores(generated_summary, original_text)\n",
    "\n",
    "    # Extract the relevant scores\n",
    "    overlap_score = scores[0]['rouge-1']['f']\n",
    "    coherence_score = scores[0]['rouge-2']['f']\n",
    "    informativeness_score = scores[0]['rouge-l']['f']\n",
    "\n",
    "    # Append the scores to the respective lists\n",
    "    overlap_scores.append(overlap_score)\n",
    "    coherence_scores.append(coherence_score)\n",
    "    informativeness_scores.append(informativeness_score)\n",
    "\n",
    "# Add the scores to the DataFrame\n",
    "df[\"Overlap_Score['rouge-1']\"] = overlap_scores\n",
    "df[\"Coherence_Score['rouge-2']\"] = coherence_scores\n",
    "df[\"Informativeness_Score['rouge-l']\"] = informativeness_scores\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "output_file = \"Summary Of (Abstrct+Claim) As single input.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "# Print the average scores\n",
    "print(\"Average Overlap Score:\", sum(overlap_scores) / len(overlap_scores))\n",
    "print(\"Average Coherence Score:\", sum(coherence_scores) / len(coherence_scores))\n",
    "print(\"Average Informativeness Score:\", sum(informativeness_scores) / len(informativeness_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449faf59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
