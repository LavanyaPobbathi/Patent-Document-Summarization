{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "567e0c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /home/lavanya/lavanya/lib/python3.7/site-packages (3.1.2)\n",
      "Collecting textstat\n",
      "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: et-xmlfile in /home/lavanya/lavanya/lib/python3.7/site-packages (from openpyxl) (1.1.0)\n",
      "Collecting pyphen (from textstat)\n",
      "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
      "Successfully installed pyphen-0.14.0 textstat-0.7.3\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl textstat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd5831dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Scores: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1630/1630 [00:00<00:00, 2139.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Flesch Reading Ease Score for Abstract Summary: 32.8268834355828\n"
     ]
    }
   ],
   "source": [
    "#abstract FRE score\n",
    "\n",
    "import pandas as pd\n",
    "import textstat\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Load the DataFrame from the Excel file\n",
    "input_file = \"GPT3.5_All_Summaries_Mergedd.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Define list to store the Flesch Reading Ease scores\n",
    "fre_scores = []\n",
    "\n",
    "# Iterate over the rows in the DataFrame with a progress bar\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
    "    # Get the generated summary\n",
    "    abstract_summary = str(row['Abstract Summary'])\n",
    "\n",
    "    # Clean the abstract summary text\n",
    "    abstract_summary = re.sub(r'[^\\x00-\\x7F]+', '', abstract_summary)\n",
    "\n",
    "    # Calculate Flesch Reading Ease score\n",
    "    fre_score = textstat.flesch_reading_ease(abstract_summary)\n",
    "\n",
    "    # Append the score to the list\n",
    "    fre_scores.append(fre_score)\n",
    "\n",
    "# Add the FRE scores to the DataFrame\n",
    "df['FRE_Score'] = fre_scores\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "output_file = \"Summary_FRE_score_abstract_gpt.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "# Print the average FRE score\n",
    "print(\"\\nAverage Flesch Reading Ease Score for Abstract Summary:\", sum(fre_scores) / len(fre_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dea43ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Scores: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1630/1630 [00:01<00:00, 966.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Flesch Reading Ease Score: 31.951306748466248\n"
     ]
    }
   ],
   "source": [
    "#claims FRE score\n",
    "\n",
    "import pandas as pd\n",
    "import textstat\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Load the DataFrame from the Excel file\n",
    "input_file = \"GPT3.5_All_Summaries_Mergedd.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Define list to store the Flesch Reading Ease scores\n",
    "fre_scores = []\n",
    "\n",
    "# Iterate over the rows in the DataFrame with a progress bar\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
    "    # Get the generated summary\n",
    "    claim_Summary = str(row['Claims Summary'])\n",
    "\n",
    "    # Clean the abstract summary text\n",
    "    claim_Summary = re.sub(r'[^\\x00-\\x7F]+', '', claim_Summary)\n",
    "\n",
    "    # Calculate Flesch Reading Ease score\n",
    "    fre_score = textstat.flesch_reading_ease(claim_Summary)\n",
    "\n",
    "    # Append the score to the list\n",
    "    fre_scores.append(fre_score)\n",
    "\n",
    "# Add the FRE scores to the DataFrame\n",
    "df['FRE_Score'] = fre_scores\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "output_file = \"Summary_FRE_score_Claims_gpt.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "# Print the average FRE score\n",
    "print(\"\\nAverage Flesch Reading Ease Score:\", sum(fre_scores) / len(fre_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "155eb1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Scores: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1630/1630 [00:00<00:00, 2355.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Flesch Reading Ease Score: 28.579871165644192\n"
     ]
    }
   ],
   "source": [
    "#claims FRE score\n",
    "\n",
    "import pandas as pd\n",
    "import textstat\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Load the DataFrame from the Excel file\n",
    "input_file = \"GPT3.5_All_Summaries_Mergedd.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Define list to store the Flesch Reading Ease scores\n",
    "fre_scores = []\n",
    "\n",
    "# Iterate over the rows in the DataFrame with a progress bar\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
    "    # Get the generated summary\n",
    "    claim_Summary = str(row['Summary(Abstract Summary+ Claim Summary)'])\n",
    "\n",
    "    # Clean the abstract summary text\n",
    "    claim_Summary = re.sub(r'[^\\x00-\\x7F]+', '', claim_Summary)\n",
    "\n",
    "    # Calculate Flesch Reading Ease score\n",
    "    fre_score = textstat.flesch_reading_ease(claim_Summary)\n",
    "\n",
    "    # Append the score to the list\n",
    "    fre_scores.append(fre_score)\n",
    "\n",
    "# Add the FRE scores to the DataFrame\n",
    "df['FRE_Score'] = fre_scores\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "output_file = \"Summary(Abstract Summary+ Claim Summary)_FRE_gpt.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "# Print the average FRE score\n",
    "print(\"\\nAverage Flesch Reading Ease Score:\", sum(fre_scores) / len(fre_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2dadbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Scores: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1630/1630 [00:00<00:00, 6275.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Flesch Reading Ease Score: 72.51726380368139\n"
     ]
    }
   ],
   "source": [
    "#claims FRE score\n",
    "\n",
    "import pandas as pd\n",
    "import textstat\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Load the DataFrame from the Excel file\n",
    "input_file = \"GPT3.5_All_Summaries_Mergedd.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Define list to store the Flesch Reading Ease scores\n",
    "fre_scores = []\n",
    "\n",
    "# Iterate over the rows in the DataFrame with a progress bar\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
    "    # Get the generated summary\n",
    "    claim_Summary = str(row['Summary of Summary(Abstract Summary+ Claim Summary)'])\n",
    "\n",
    "    # Clean the abstract summary text\n",
    "    claim_Summary = re.sub(r'[^\\x00-\\x7F]+', '', claim_Summary)\n",
    "\n",
    "    # Calculate Flesch Reading Ease score\n",
    "    fre_score = textstat.flesch_reading_ease(claim_Summary)\n",
    "\n",
    "    # Append the score to the list\n",
    "    fre_scores.append(fre_score)\n",
    "\n",
    "# Add the FRE scores to the DataFrame\n",
    "df['FRE_Score'] = fre_scores\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "output_file = \"Summary of Summary(Abstract Summary+ Claim Summary)_FRE_gpt.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "# Print the average FRE score\n",
    "print(\"\\nAverage Flesch Reading Ease Score:\", sum(fre_scores) / len(fre_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a0c966a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Scores: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1630/1630 [00:00<00:00, 2246.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Flesch Reading Ease Score: 31.201196319018397\n"
     ]
    }
   ],
   "source": [
    "#claims FRE score\n",
    "\n",
    "import pandas as pd\n",
    "import textstat\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Load the DataFrame from the Excel file\n",
    "input_file = \"GPT3.5_All_Summaries_Mergedd.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Define list to store the Flesch Reading Ease scores\n",
    "fre_scores = []\n",
    "\n",
    "# Iterate over the rows in the DataFrame with a progress bar\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
    "    # Get the generated summary\n",
    "    claim_Summary = str(row['Summary Of (Abstrct+Claim) As single input'])\n",
    "\n",
    "    # Clean the abstract summary text\n",
    "    claim_Summary = re.sub(r'[^\\x00-\\x7F]+', '', claim_Summary)\n",
    "\n",
    "    # Calculate Flesch Reading Ease score\n",
    "    fre_score = textstat.flesch_reading_ease(claim_Summary)\n",
    "\n",
    "    # Append the score to the list\n",
    "    fre_scores.append(fre_score)\n",
    "\n",
    "# Add the FRE scores to the DataFrame\n",
    "df['FRE_Score'] = fre_scores\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "output_file = \"Summary Of (Abstrct+Claim) As single input_FRE_gpt.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "# Print the average FRE score\n",
    "print(\"\\nAverage Flesch Reading Ease Score:\", sum(fre_scores) / len(fre_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c028c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
