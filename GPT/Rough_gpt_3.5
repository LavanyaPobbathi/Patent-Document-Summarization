#### Combined summary Rogue Score
from rouge import Rouge
import pandas as pd
from tqdm import tqdm
import torch
import re

# Load the DataFrame from the Excel file
input_file = "GPT3.5_All_Summaries_Mergedd.xlsx"
df = pd.read_excel(input_file)

# Create a Rouge object
rouge = Rouge()

# Define lists to store the overlap, coherence, and informativeness scores
overlap_scores = []
coherence_scores = []
informativeness_scores = []

# Set device to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Iterate over the rows in the DataFrame with a progress bar
for index, row in tqdm(df.iterrows(), total=len(df), desc="Calculating Scores"):
    # Get the original text and the generated summary
    abstract = row['Abstract Summary']
    claim = row['Claims Summary']

    # Clean the abstract and claims text
    abstract = re.sub(r'[^\x00-\x7F]+', '', str(abstract))
    claims = re.sub(r'[^\x00-\x7F]+', '', str(claim))

    # Combine the cleaned abstract and claims
    original_text = abstract+' '+claims
    generated_summary = str(row['Summary(Abstract Summary+ Claim Summary)'])

    # Calculate ROUGE scores
    scores = rouge.get_scores(generated_summary, original_text)

    # Extract the relevant scores
    overlap_score = scores[0]['rouge-1']['f']
    coherence_score = scores[0]['rouge-2']['f']
    informativeness_score = scores[0]['rouge-l']['f']

    # Append the scores to the respective lists
    overlap_scores.append(overlap_score)
    coherence_scores.append(coherence_score)
    informativeness_scores.append(informativeness_score)

# Add the scores to the DataFrame
df["Overlap_Score['rouge-1']"] = overlap_scores
df["Coherence_Score['rouge-2']"] = coherence_scores
df["Informativeness_Score['rouge-l']"] = informativeness_scores

# Save the updated DataFrame to a new Excel file
output_file = "Summary(Abstract Summary+ Claim Summary).xlsx"
df.to_excel(output_file, index=False)

# Print the average scores
print("Average Overlap Score:", sum(overlap_scores) / len(overlap_scores))
print("Average Coherence Score:", sum(coherence_scores) / len(coherence_scores))
print("Average Informativeness Score:", sum(informativeness_scores) / len(informativeness_scores))


#### Claims Rogue Score
from rouge import Rouge
import pandas as pd
from tqdm import tqdm
import torch
import re

# Load the DataFrame from the Excel file
input_file = "GPT3.5_All_Summaries_Mergedd.xlsx"
df = pd.read_excel(input_file)

# Create a Rouge object
rouge = Rouge()

# Define lists to store the overlap, coherence, and informativeness scores
overlap_scores = []
coherence_scores = []
informativeness_scores = []

# Set device to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Iterate over the rows in the DataFrame with a progress bar
for index, row in tqdm(df.iterrows(), total=len(df), desc="Calculating Scores"):
    # Get the original text and the generated summary
    claim = row['Summary(Abstract Summary+ Claim Summary)']

    # Clean the abstract and claims text
    claims = re.sub(r'[^\x00-\x7F]+', '', str(claim))

    # Combine the cleaned abstract and claims
    original_text = claims
    generated_summary = str(row['Summary of Summary(Abstract Summary+ Claim Summary)'])

    # Calculate ROUGE scores
    scores = rouge.get_scores(generated_summary, original_text)

    # Extract the relevant scores
    overlap_score = scores[0]['rouge-1']['f']
    coherence_score = scores[0]['rouge-2']['f']
    informativeness_score = scores[0]['rouge-l']['f']

    # Append the scores to the respective lists
    overlap_scores.append(overlap_score)
    coherence_scores.append(coherence_score)
    informativeness_scores.append(informativeness_score)

# Add the scores to the DataFrame
df["Overlap_Score['rouge-1']"] = overlap_scores
df["Coherence_Score['rouge-2']"] = coherence_scores
df["Informativeness_Score['rouge-l']"] = informativeness_scores

# Save the updated DataFrame to a new Excel file
output_file = "Summary of Summary(Abstract Summary+ Claim Summary).xlsx"
df.to_excel(output_file, index=False)

# Print the average scores
print("Average Overlap Score:", sum(overlap_scores) / len(overlap_scores))
print("Average Coherence Score:", sum(coherence_scores) / len(coherence_scores))
print("Average Informativeness Score:", sum(informativeness_scores) / len(informativeness_scores))

#### Combined summary Rogue Score
from rouge import Rouge
import pandas as pd
from tqdm import tqdm
import torch
import re

# Load the DataFrame from the Excel file
input_file = "GPT3.5_All_Summaries_Mergedd.xlsx"
df = pd.read_excel(input_file)

# Create a Rouge object
rouge = Rouge()

# Define lists to store the overlap, coherence, and informativeness scores
overlap_scores = []
coherence_scores = []
informativeness_scores = []

# Set device to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Iterate over the rows in the DataFrame with a progress bar
for index, row in tqdm(df.iterrows(), total=len(df), desc="Calculating Scores"):
    # Get the original text and the generated summary
    abstract = row['Abstract']
    claim = row['Claims']

    # Clean the abstract and claims text
    abstract = re.sub(r'[^\x00-\x7F]+', '', str(abstract))
    claims = re.sub(r'[^\x00-\x7F]+', '', str(claim))

    # Combine the cleaned abstract and claims
    original_text = abstract+' '+claims
    generated_summary = str(row['Summary Of (Abstrct+Claim) As single input'])

    # Calculate ROUGE scores
    scores = rouge.get_scores(generated_summary, original_text)

    # Extract the relevant scores
    overlap_score = scores[0]['rouge-1']['f']
    coherence_score = scores[0]['rouge-2']['f']
    informativeness_score = scores[0]['rouge-l']['f']

    # Append the scores to the respective lists
    overlap_scores.append(overlap_score)
    coherence_scores.append(coherence_score)
    informativeness_scores.append(informativeness_score)

# Add the scores to the DataFrame
df["Overlap_Score['rouge-1']"] = overlap_scores
df["Coherence_Score['rouge-2']"] = coherence_scores
df["Informativeness_Score['rouge-l']"] = informativeness_scores

# Save the updated DataFrame to a new Excel file
output_file = "Summary Of (Abstrct+Claim) As single input.xlsx"
df.to_excel(output_file, index=False)

# Print the average scores
print("Average Overlap Score:", sum(overlap_scores) / len(overlap_scores))
print("Average Coherence Score:", sum(coherence_scores) / len(coherence_scores))
print("Average Informativeness Score:", sum(informativeness_scores) / len(informativeness_scores))