{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f04edec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement sentence_bleu (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for sentence_bleu\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc76a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Scores:   0%|                                                                                                                      | 0/1630 [00:00<?, ?it/s]/home/lavanya/lavanya/lib/python3.7/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/lavanya/lavanya/lib/python3.7/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/lavanya/lavanya/lib/python3.7/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Calculating Scores: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1630/1630 [00:01<00:00, 1498.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average BLEU Score: 0.20438529649510317\n"
     ]
    }
   ],
   "source": [
    "#abstract bleu score\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Load the DataFrame from the Excel file\n",
    "input_file = \"GPT3.5_All_Summaries_Mergedd.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Define lists to store the scores\n",
    "bleu_scores = []\n",
    "fluency_scores = []\n",
    "accuracy_scores = []\n",
    "readability_scores = []\n",
    "relevance_scores = []\n",
    "novelty_scores = []\n",
    "style_scores = []\n",
    "\n",
    "# Iterate over the rows in the DataFrame with a progress bar\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
    "    # Get the original text and the generated summary\n",
    "    abstract = row['Abstract']\n",
    "    #claims = row['Claims']\n",
    "\n",
    "    # Clean the abstract and claims text\n",
    "    abstract = re.sub(r'[^\\x00-\\x7F]+', '', str(abstract))\n",
    "    #claims = re.sub(r'[^\\x00-\\x7F]+', '', str(claims))\n",
    "\n",
    "    # Combine the cleaned abstract and claims\n",
    "    original_text = abstract\n",
    "    generated_summary = str(row['Abstract Summary'])\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    reference = [original_text.split()]\n",
    "    candidate = generated_summary.split()\n",
    "    bleu_score = sentence_bleu(reference, candidate)\n",
    "\n",
    "    # Append the BLEU score\n",
    "    bleu_scores.append(bleu_score)\n",
    "\n",
    "# Add the scores to the DataFrame\n",
    "df['BLEU_Score'] = bleu_scores\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "output_file = \"Summary_Bleu_score_abstract_gpt.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "# Print the average scores\n",
    "print(\"\\nAverage BLEU Score:\", sum(bleu_scores) / len(bleu_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6507a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Scores: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1630/1630 [00:04<00:00, 347.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average BLEU Score: 0.11076683882772145\n"
     ]
    }
   ],
   "source": [
    "#claims bleu score\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Load the DataFrame from the Excel file\n",
    "input_file = \"GPT3.5_All_Summaries_Mergedd.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Define lists to store the scores\n",
    "bleu_scores = []\n",
    "fluency_scores = []\n",
    "accuracy_scores = []\n",
    "readability_scores = []\n",
    "relevance_scores = []\n",
    "novelty_scores = []\n",
    "style_scores = []\n",
    "\n",
    "# Iterate over the rows in the DataFrame with a progress bar\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
    "    # Get the original text and the generated summary\n",
    "    #abstract = row['Abstract']\n",
    "    claims = row['Claims']\n",
    "\n",
    "    # Clean the abstract and claims text\n",
    "    #abstract = re.sub(r'[^\\x00-\\x7F]+', '', str(abstract))\n",
    "    claims = re.sub(r'[^\\x00-\\x7F]+', '', str(claims))\n",
    "\n",
    "    # Combine the cleaned abstract and claims\n",
    "    original_text = claims\n",
    "    generated_summary = str(row['Claims Summary'])\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    reference = [original_text.split()]\n",
    "    candidate = generated_summary.split()\n",
    "    bleu_score = sentence_bleu(reference, candidate)\n",
    "\n",
    "    # Append the BLEU score\n",
    "    bleu_scores.append(bleu_score)\n",
    "\n",
    "# Add the scores to the DataFrame\n",
    "df['BLEU_Score'] = bleu_scores\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "output_file = \"Summary_bleu_score_Claims_gpt.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "# Print the average scores\n",
    "print(\"\\nAverage BLEU Score:\", sum(bleu_scores) / len(bleu_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "858a7972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Scores: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1630/1630 [00:02<00:00, 779.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average BLEU Score: 0.12151850275164301\n"
     ]
    }
   ],
   "source": [
    "#combined bleu score\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Load the DataFrame from the Excel file\n",
    "input_file = \"GPT3.5_All_Summaries_Mergedd.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Define lists to store the scores\n",
    "bleu_scores = []\n",
    "fluency_scores = []\n",
    "accuracy_scores = []\n",
    "readability_scores = []\n",
    "relevance_scores = []\n",
    "novelty_scores = []\n",
    "style_scores = []\n",
    "\n",
    "# Iterate over the rows in the DataFrame with a progress bar\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
    "    # Get the original text and the generated summary\n",
    "    abstract = row['Abstract Summary']\n",
    "    claims = row['Claims Summary']\n",
    "\n",
    "    # Clean the abstract and claims text\n",
    "    abstract = re.sub(r'[^\\x00-\\x7F]+', '', str(abstract))\n",
    "    claims = re.sub(r'[^\\x00-\\x7F]+', '', str(claims))\n",
    "\n",
    "    # Combine the cleaned abstract and claims\n",
    "    original_text = abstract+' '+claims\n",
    "    generated_summary = str(row['Summary(Abstract Summary+ Claim Summary)'])\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    reference = [original_text.split()]\n",
    "    candidate = generated_summary.split()\n",
    "    bleu_score = sentence_bleu(reference, candidate)\n",
    "\n",
    "    # Append the BLEU score\n",
    "    bleu_scores.append(bleu_score)\n",
    "\n",
    "# Add the scores to the DataFrame\n",
    "df['BLEU_Score'] = bleu_scores\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "output_file = \"Summary(Abstract Summary+ Claim Summary)_bleu_gpt.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "# Print the average scores\n",
    "print(\"\\nAverage BLEU Score:\", sum(bleu_scores) / len(bleu_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d37d6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Scores: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1630/1630 [00:00<00:00, 2262.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average BLEU Score: 0.01182715152185676\n"
     ]
    }
   ],
   "source": [
    "#claims bleu score\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Load the DataFrame from the Excel file\n",
    "input_file = \"GPT3.5_All_Summaries_Mergedd.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Define lists to store the scores\n",
    "bleu_scores = []\n",
    "fluency_scores = []\n",
    "accuracy_scores = []\n",
    "readability_scores = []\n",
    "relevance_scores = []\n",
    "novelty_scores = []\n",
    "style_scores = []\n",
    "\n",
    "# Iterate over the rows in the DataFrame with a progress bar\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
    "    # Get the original text and the generated summary\n",
    "    #abstract = row['Abstract']\n",
    "    claims = row['Summary(Abstract Summary+ Claim Summary)']\n",
    "\n",
    "    # Clean the abstract and claims text\n",
    "    #abstract = re.sub(r'[^\\x00-\\x7F]+', '', str(abstract))\n",
    "    claims = re.sub(r'[^\\x00-\\x7F]+', '', str(claims))\n",
    "\n",
    "    # Combine the cleaned abstract and claims\n",
    "    original_text = claims\n",
    "    generated_summary = str(row['Summary of Summary(Abstract Summary+ Claim Summary)'])\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    reference = [original_text.split()]\n",
    "    candidate = generated_summary.split()\n",
    "    bleu_score = sentence_bleu(reference, candidate)\n",
    "\n",
    "    # Append the BLEU score\n",
    "    bleu_scores.append(bleu_score)\n",
    "\n",
    "# Add the scores to the DataFrame\n",
    "df['BLEU_Score'] = bleu_scores\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "output_file = \"Summary of Summary(Abstract Summary+ Claim Summary)_bleu_gpt.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "# Print the average scores\n",
    "print(\"\\nAverage BLEU Score:\", sum(bleu_scores) / len(bleu_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66ac3b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Scores: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1630/1630 [00:03<00:00, 423.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average BLEU Score: 0.00458567459044246\n"
     ]
    }
   ],
   "source": [
    "#combined bleu score\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Load the DataFrame from the Excel file\n",
    "input_file = \"GPT3.5_All_Summaries_Mergedd.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Define lists to store the scores\n",
    "bleu_scores = []\n",
    "fluency_scores = []\n",
    "accuracy_scores = []\n",
    "readability_scores = []\n",
    "relevance_scores = []\n",
    "novelty_scores = []\n",
    "style_scores = []\n",
    "\n",
    "# Iterate over the rows in the DataFrame with a progress bar\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
    "    # Get the original text and the generated summary\n",
    "    abstract = row['Abstract']\n",
    "    claims = row['Claims']\n",
    "\n",
    "    # Clean the abstract and claims text\n",
    "    abstract = re.sub(r'[^\\x00-\\x7F]+', '', str(abstract))\n",
    "    claims = re.sub(r'[^\\x00-\\x7F]+', '', str(claims))\n",
    "\n",
    "    # Combine the cleaned abstract and claims\n",
    "    original_text = abstract+' '+claims\n",
    "    generated_summary = str(row['Summary Of (Abstrct+Claim) As single input'])\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    reference = [original_text.split()]\n",
    "    candidate = generated_summary.split()\n",
    "    bleu_score = sentence_bleu(reference, candidate)\n",
    "\n",
    "    # Append the BLEU score\n",
    "    bleu_scores.append(bleu_score)\n",
    "\n",
    "# Add the scores to the DataFrame\n",
    "df['BLEU_Score'] = bleu_scores\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "output_file = \"Summary Of (Abstrct+Claim) As single input_bleu_gpt.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "# Print the average scores\n",
    "print(\"\\nAverage BLEU Score:\", sum(bleu_scores) / len(bleu_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d72fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
