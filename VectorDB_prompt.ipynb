{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LavanyaPobbathi/Patent-Document-Summarization/blob/main/VectorDB_prompt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYJPzvnhc-H_",
        "outputId": "1775fe6e-72f5-4b63-bd0c-b59469e8413e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.240-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.19)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.13-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.11 (from langchain)\n",
            "  Downloading langsmith-0.0.14-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.11)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, langsmith, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.13 langchain-0.0.240 langsmith-0.0.14 marshmallow-3.20.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter"
      ],
      "metadata": {
        "id": "GQbibwcOd8JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "import json\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, page_content, metadata):\n",
        "        self.page_content = page_content\n",
        "        self.metadata = metadata\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Document(page_content={self.page_content}, metadata={self.metadata})\"\n",
        "\n",
        "def get_local_chunks(json_file):\n",
        "    source_chunks = []\n",
        "    splitter = CharacterTextSplitter(separator=\" \", chunk_size=1024, chunk_overlap=0)\n",
        "    with open(json_file, \"r\") as f:\n",
        "        json_data = json.load(f)\n",
        "        for chunk in splitter.split_text(json_data[\"abstract\"]):\n",
        "            source_chunks.append(Document(page_content=chunk, metadata={\"source\": json_file}))\n",
        "    return source_chunks\n",
        "\n",
        "\n",
        "json_file = Path(\"./US10048858.json\")\n",
        "source_chunks = get_local_chunks(json_file)"
      ],
      "metadata": {
        "id": "Rh9gT3l8ew-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyca2ptagdbQ",
        "outputId": "e818900f-7c36-49ab-cdd5-73e24146c5fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=A method and apparatus for swipe shift photo browsing, wherein a first image is displayed on a touch sensitive display, a transition for the first image and an adjacent image is automatically created or determined, a swipe gesture of the user is recognized with the touch sensitive display, and the image shown on the display is shifted. The shifting comprises displaying the transition of the first image and of a second image synchronized with the speed and direction of the swipe, wherein the second image is the adjacent image., metadata={'source': PosixPath('US10048858.json')})]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKrDPg6T0lWg",
        "outputId": "1b476673-266e-4d56-845a-772cccd97ff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.docstore.document import Document\n",
        "import requests\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.prompts import PromptTemplate\n",
        "import pathlib\n",
        "import subprocess\n",
        "import tempfile"
      ],
      "metadata": {
        "id": "WfG83wcfh6cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qwCRr1dH0r2K",
        "outputId": "e8a0b18b-87ae-4b5c-98cd-3cef06a7620e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.2-py3-none-any.whl (399 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.3/399.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.5.3)\n",
            "Collecting requests>=2.28 (from chromadb)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.11)\n",
            "Collecting chroma-hnswlib==0.7.1 (from chromadb)\n",
            "  Downloading chroma-hnswlib-0.7.1.tar.gz (30 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fastapi<0.100.0,>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.23.1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.22.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.0.1-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.7.1)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.13.2 (from chromadb)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.65.0)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.3.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.0)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi<0.100.0,>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.11.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (1.26.16)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.6)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.1.2)\n",
            "Building wheels for collected packages: chroma-hnswlib, pypika\n",
            "  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.1-cp310-cp310-linux_x86_64.whl size=2272521 sha256=376ace8b6646e53d431afc7ce727cb8a4bcb38f674426f20c76222f0434f5e69\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/f2/d2/3f32228e9f4713a9f32a468de8bbc3c642f7805ebef888418b\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=16f908047c6f6a89ff2bf36569efc5f8386ead059d4270a1adea37c630c781c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built chroma-hnswlib pypika\n",
            "Installing collected packages: tokenizers, pypika, monotonic, websockets, uvloop, requests, python-dotenv, pulsar-client, overrides, humanfriendly, httptools, h11, chroma-hnswlib, backoff, watchfiles, uvicorn, starlette, posthog, coloredlogs, onnxruntime, fastapi, chromadb\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 chroma-hnswlib-0.7.1 chromadb-0.4.2 coloredlogs-15.0.1 fastapi-0.99.1 h11-0.14.0 httptools-0.6.0 humanfriendly-10.0 monotonic-1.6 onnxruntime-1.15.1 overrides-7.3.1 posthog-3.0.1 pulsar-client-3.2.0 pypika-0.48.9 python-dotenv-1.0.0 requests-2.31.0 starlette-0.27.0 tokenizers-0.13.3 uvicorn-0.23.1 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFaJvd1a1UaU",
        "outputId": "5799db83-d598-421a-e8fe-b29097f3785c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.7 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "import json\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, page_content, metadata):\n",
        "        self.page_content = page_content\n",
        "        self.metadata = metadata\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Document(page_content={self.page_content}, metadata={self.metadata})\"\n",
        "\n",
        "def get_local_chunks(json_file):\n",
        "    source_chunks = []\n",
        "    splitter = CharacterTextSplitter(separator=\" \", chunk_size=1024, chunk_overlap=0)\n",
        "    with open(json_file, \"r\") as f:\n",
        "        json_data = json.load(f)\n",
        "        claims_text = json_data.get(\"abstract\", \"\")\n",
        "        source_chunks = splitter.split_text(claims_text)\n",
        "        # Create a Document object for each chunk\n",
        "        source_chunks = [Document(page_content=chunk, metadata={\"source\": str(Path(json_file))}) for chunk in source_chunks]\n",
        "    return source_chunks\n",
        "\n",
        "\n",
        "json_file = Path(\"./US10048858.json\")\n",
        "source_chunks = get_local_chunks(json_file)\n"
      ],
      "metadata": {
        "id": "dOUSJa-f23kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRxVWfLrRSn2",
        "outputId": "cc3f342c-5cd3-47f4-bf95-d943846422e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=A method and apparatus for swipe shift photo browsing, wherein a first image is displayed on a touch sensitive display, a transition for the first image and an adjacent image is automatically created or determined, a swipe gesture of the user is recognized with the touch sensitive display, and the image shown on the display is shifted. The shifting comprises displaying the transition of the first image and of a second image synchronized with the speed and direction of the swipe, wherein the second image is the adjacent image., metadata={'source': 'US10048858.json'})]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_index = Chroma.from_documents(source_chunks, OpenAIEmbeddings(openai_api_key='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'))\n"
      ],
      "metadata": {
        "id": "lzu53vRjibPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "prompt_template = \"\"\"Use the context below to write a 400 word blog post about the topic below:\n",
        "    Context: {context}\n",
        "    Topic: {topic}\n",
        "    Blog post:\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"topic\"])\n",
        "\n",
        "llm = OpenAI(openai_api_key='', temperature=0)\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=PROMPT)"
      ],
      "metadata": {
        "id": "8LfdgH313API"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_blog_post(topic):\n",
        "    docs = search_index.similarity_search(topic, k=4)\n",
        "    inputs = [{\"context\": doc.page_content, \"topic\": topic} for doc in docs]\n",
        "    print(chain.apply(inputs))"
      ],
      "metadata": {
        "id": "nuvrFNZq4w-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_blog_post(\"Can you generate a summary of this US10048858 google patent document\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLVqOwCW41wK",
        "outputId": "3270069a-c685-457b-ce8e-b6580a860bc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'text': '\\n\\nHave you ever wondered how the images you see on your phone or computer screen transition from one to the next? It’s a process that’s been made possible by the US10048858 google patent document. This document outlines an apparatus that can automatically determine and/or create a transition for images based on transition data.\\n\\nThe apparatus consists of a touch sensitive display, a memory configured to store images, and a processor configured to cause the display of a first image. The transition data is comprised in or linked with the first image and the adjacent image, and it includes information such as an edited version of the respective image being available, the respective image being part of a sequence of images, and the respective image being part of a combination image.\\n\\nWhen the transition data shows that an edited version of the image is available, the transition comprises sliding the image into the display synchronized with the speed and direction of the swipe and blending into at least one edited version of the image as the image is fully on the display. When the transition data does not show that an edited version of the image is available, the transition comprises sliding the image into the display synchronized with the speed and direction of the swipe and blending into the adjacent image as the image is fully'}, {'text': '\\n\\nThe US10048858 Google patent document is an apparatus for automatically determining and/or creating a transition between at least two adjacent images. This apparatus is comprised of a processor, a memory, and a camera unit. The processor is configured to cause the transition to be displayed in a variety of ways, such as sliding the image out of or into the display synchronized with the speed and direction of the swipe, or sliding a linked image into the display synchronized with the speed and direction of the swipe and blending into the image as the linked image is fully on the display.\\n\\nThe US10048858 Google patent document is a great tool for those who want to create smooth transitions between images. It allows users to easily and quickly create transitions between images, without having to manually create them. This patent document is especially useful for those who are creating videos or presentations, as it allows them to quickly and easily create transitions between images.\\n\\nThe US10048858 Google patent document is also useful for those who are creating interactive applications. By using the processor, memory, and camera unit, users can create transitions between images that are triggered by user input. This allows users to create interactive applications that are more engaging and visually appealing.\\n\\nOverall, the US10048858'}, {'text': '\\n\\nHave you ever wanted to create a smooth transition between two images on a touch-sensitive display? If so, you’ll be interested to learn about the US10048858 Google patent document. This document outlines a method for displaying a transition between two images on a touch-sensitive display.\\n\\nThe method begins by displaying a first image on the display. Then, the transition between the first image and an adjacent image is automatically determined or created based on transition data comprised in or linked with the first image and the adjacent image. This transition data includes information such as an edited version of the respective image being available, the respective image’s location, and the respective image’s capture time.\\n\\nThe transition between the two images is achieved by sliding the image into the display, synchronized with the speed and direction of the swipe. As the image is sliding into the display, parts from overlapping images are added to the image and blended into an image combined from the overlapping images. This creates a smooth transition between the two images.\\n\\nThe method also includes the ability to switch into a map view and zoom in or out of the map and from or to the location the image was captured at. As the map view is fully on the display, it'}, {'text': '\\n\\nThe US10048858 Google patent document is an apparatus that allows for the transition of images on a touch sensitive display. This patent document outlines the process of recognizing a swipe gesture of the user with the touch sensitive display and shifting the image shown on the display. The shifting of the image is done in a synchronized manner with the speed and direction of the swipe.\\n\\nThe patent document also outlines two different types of transitions. The first transition is when the image is part of a sequence of images. In this case, the image is slid into the display in a synchronized manner with the speed and direction of the swipe. The image is then blended into an image of the sequence of images as the image is fully on the display.\\n\\nThe second transition is when the image is part of a combined image. In this case, a larger combined image is slid into the display in a synchronized manner with the speed and direction of the swipe until the edge of the image is reached.\\n\\nOverall, the US10048858 Google patent document outlines an apparatus that allows for the transition of images on a touch sensitive display. This patent document outlines two different types of transitions, one for when the image is part of a sequence of images and one for when the image is part'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "another"
      ],
      "metadata": {
        "id": "ZU_YSvh39Jd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "import json\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, page_content, metadata):\n",
        "        self.page_content = page_content\n",
        "        self.metadata = metadata\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Document(page_content={self.page_content}, metadata={self.metadata})\"\n",
        "\n",
        "def get_local_chunks(json_file, num_chunks):\n",
        "    source_chunks = []\n",
        "    splitter = CharacterTextSplitter(separator=\" \", chunk_size=1024, chunk_overlap=0)\n",
        "    with open(json_file, \"r\") as f:\n",
        "        json_data = json.load(f)\n",
        "        abstract = str(json_data[\"abstract\"])\n",
        "        introduction = str(json_data[\"title\"])\n",
        "        body = str(json_data[\"claims\"])\n",
        "\n",
        "        # Process abstract section\n",
        "        abstract_chunks = splitter.split_text(abstract)\n",
        "        for i in range(min(num_chunks, len(abstract_chunks))):\n",
        "            chunk = abstract_chunks[i]\n",
        "            source_chunks.append(Document(page_content=chunk, metadata={\"source\": str(Path(json_file))}))\n",
        "\n",
        "        # Process introduction section\n",
        "        introduction_chunks = splitter.split_text(introduction)\n",
        "        for i in range(min(num_chunks, len(introduction_chunks))):\n",
        "            chunk = introduction_chunks[i]\n",
        "            source_chunks.append(Document(page_content=chunk, metadata={\"source\": str(Path(json_file))}))\n",
        "\n",
        "        # Process body section\n",
        "        body_chunks = splitter.split_text(body)\n",
        "        for i in range(min(num_chunks, len(body_chunks))):\n",
        "            chunk = body_chunks[i]\n",
        "            source_chunks.append(Document(page_content=chunk, metadata={\"source\": str(Path(json_file))}))\n",
        "\n",
        "    return source_chunks\n",
        "\n",
        "json_file = Path(\"./US10048858.json\")\n",
        "num_chunks = 4  # Adjust this number as per your requirement\n",
        "source_chunks = get_local_chunks(json_file, num_chunks)"
      ],
      "metadata": {
        "id": "tfFgocax9InE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_index = Chroma.from_documents(source_chunks, OpenAIEmbeddings(openai_api_key='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'))\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "prompt_template = \"\"\"Use the context below to write a 400 word blog post about the topic below:\n",
        "    Context: {context}\n",
        "    Topic: {topic}\n",
        "    Blog post:\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"topic\"])\n",
        "\n",
        "llm = OpenAI(openai_api_key='', temperature=0)\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=PROMPT)\n",
        "\n"
      ],
      "metadata": {
        "id": "w-71PkNK9P9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_blog_post(topic):\n",
        "    docs = search_index.similarity_search(topic, k=2)\n",
        "    inputs = [{\"context\": doc.page_content, \"topic\": topic} for doc in docs]\n",
        "    print(chain.apply(inputs))\n",
        "\n",
        "generate_blog_post(\"Can you generate a summary of this US10048858 google patent document\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EP2hEmY9U28",
        "outputId": "e760fea0-266c-4d65-eda5-4498b2efae3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'text': '\\n\\nHave you ever wondered how the images you see on your phone or computer screen transition from one to the next? It’s a process that’s been made possible by the US10048858 google patent document. This document outlines an apparatus that can automatically determine and/or create a transition for images based on transition data.\\n\\nThe apparatus consists of a touch sensitive display, a memory configured to store images, and a processor configured to cause the display of a first image. The transition data is comprised in or linked with the first image and the adjacent image, and it includes information such as an edited version of the respective image being available, the respective image being part of a sequence of images, and the respective image being part of a combination image.\\n\\nWhen the transition data shows that an edited version of the image is available, the transition comprises sliding the image into the display synchronized with the speed and direction of the swipe and blending into at least one edited version of the image as the image is fully on the display. When the transition data does not show that an edited version of the image is available, the transition comprises sliding the image into the display synchronized with the speed and direction of the swipe and blending into the adjacent image as the image is fully'}, {'text': '\\n\\nThe US10048858 Google patent document is an apparatus for automatically determining and/or creating a transition between at least two adjacent images. This apparatus is comprised of a processor, a memory, and a camera unit. The processor is configured to cause the transition to be displayed in a variety of ways, such as sliding the image out of or into the display synchronized with the speed and direction of the swipe, or sliding a linked image into the display synchronized with the speed and direction of the swipe and blending into the image as the linked image is fully on the display.\\n\\nThis patent document is a great example of how technology is being used to create a more seamless transition between images. By automatically determining and/or creating a transition between two adjacent images, the user experience is improved and the process of transitioning between images is made easier. This patent document also allows for the retrieval of existing transitions from the memory, which can be used to create a more dynamic and interesting transition between images.\\n\\nThe camera unit is also an important part of this patent document, as it allows for the capturing of images with the camera unit. This allows for the user to capture images in real-time, which can then be used to create a transition between images. This is a great'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unknown input"
      ],
      "metadata": {
        "id": "9F1jrTDbDkLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "import json\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, page_content, metadata):\n",
        "        self.page_content = page_content\n",
        "        self.metadata = metadata\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Document(page_content={self.page_content}, metadata={self.metadata})\"\n",
        "\n",
        "def get_local_chunks(json_file, num_chunks):\n",
        "    source_chunks = []\n",
        "    splitter = CharacterTextSplitter(separator=\" \", chunk_size=1024, chunk_overlap=0)\n",
        "    with open(json_file, \"r\") as f:\n",
        "        json_data = json.load(f)\n",
        "        abstract = str(json_data[\"abstract\"])\n",
        "        introduction = str(json_data[\"title\"])\n",
        "        body = str(json_data[\"claims\"])\n",
        "\n",
        "        # Process abstract section\n",
        "        abstract_chunks = splitter.split_text(abstract)\n",
        "        for i in range(min(num_chunks, len(abstract_chunks))):\n",
        "            chunk = abstract_chunks[i]\n",
        "            source_chunks.append(Document(page_content=chunk, metadata={\"source\": str(Path(json_file))}))\n",
        "\n",
        "        # Process introduction section\n",
        "        introduction_chunks = splitter.split_text(introduction)\n",
        "        for i in range(min(num_chunks, len(introduction_chunks))):\n",
        "            chunk = introduction_chunks[i]\n",
        "            source_chunks.append(Document(page_content=chunk, metadata={\"source\": str(Path(json_file))}))\n",
        "\n",
        "        # Process body section\n",
        "        body_chunks = splitter.split_text(body)\n",
        "        for i in range(min(num_chunks, len(body_chunks))):\n",
        "            chunk = body_chunks[i]\n",
        "            source_chunks.append(Document(page_content=chunk, metadata={\"source\": str(Path(json_file))}))\n",
        "\n",
        "    return source_chunks\n",
        "\n",
        "json_file = Path(\"./US10048858.json\")\n",
        "num_chunks = 4  # Adjust this number as per your requirement\n",
        "source_chunks = get_local_chunks(json_file, num_chunks)\n",
        "\n",
        "search_index = Chroma.from_documents(source_chunks, OpenAIEmbeddings(openai_api_key='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'))\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "prompt_template = \"\"\"Use the context below to write a 400 word blog post about the topic below:\n",
        "    Context: {context}\n",
        "    Topic: {topic}\n",
        "    Blog post:\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"topic\"])\n",
        "\n",
        "llm = OpenAI(openai_api_key='', temperature=0)\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=PROMPT)\n",
        "\n",
        "def generate_blog_post(topic):\n",
        "    docs = search_index.similarity_search(topic, k=2)\n",
        "    inputs = [{\"context\": doc.page_content, \"topic\": topic} for doc in docs]\n",
        "    print(chain.apply(inputs))\n",
        "\n",
        "generate_blog_post(\"give me the summary of claims of this US10048858\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHk_8JZSDlOf",
        "outputId": "f67a9217-a20f-4428-9849-10b7b49043e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'text': '\\n\\nThe US10048858 patent claims an apparatus that includes a touch sensitive display, a memory configured to store images, and a processor configured to cause the display of a first image on the touch sensitive display. The processor is also configured to automatically determine and/or create a transition for the first image and an adjacent image based on transition data comprised in or linked with the first image and the adjacent image. The transition data includes information such as an edited version of the respective image being available, the respective image being part of a sequence of images, and the respective image being part of a combination image. \\n\\nThe transition comprises, when the transition data shows that an edited version of the image is available, sliding the image into the display synchronized with the speed and direction of the swipe and blending into at least one edited version of the image as the image is fully on the display. Alternatively, when the transition data shows that the image is part of a sequence of images, the transition comprises sliding the image into the display synchronized with the speed and direction of the swipe and blending into the next image in the sequence as the image is fully on the display. Finally, when the transition data shows that the image is part of a combination image, the transition comprises sliding the image into'}, {'text': '\\n\\nThe US10048858 patent claims an apparatus that includes a touch sensitive display, a memory configured to store images, and a processor configured to cause the display of a first image on the touch sensitive display. The processor is also configured to automatically determine and/or create a transition for the first image and an adjacent image based on transition data comprised in or linked with the first image and the adjacent image. The transition data includes information such as an edited version of the respective image being available, the respective image being part of a sequence of images, and the respective image being part of a combination image. \\n\\nThe transition comprises, when the transition data shows that an edited version of the image is available, sliding the image into the display synchronized with the speed and direction of the swipe and blending into at least one edited version of the image as the image is fully on the display. Alternatively, when the transition data shows that the image is part of a sequence of images, the transition comprises sliding the image into the display synchronized with the speed and direction of the swipe and blending into the next image in the sequence as the image is fully on the display. Finally, when the transition data shows that the image is part of a combination image, the transition comprises sliding the image into'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9WPXu0UuT3x",
        "outputId": "fbd979fd-467a-4cf5-acff-d9bff8d0fdee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By taking all the text docs"
      ],
      "metadata": {
        "id": "PH4Q0gpXFILz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, page_content, metadata):\n",
        "        self.page_content = page_content\n",
        "        self.metadata = metadata\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Document(page_content={self.page_content}, metadata={self.metadata})\"\n",
        "\n",
        "def get_local_chunks(json_file, num_chunks):\n",
        "    source_chunks = []\n",
        "    splitter = CharacterTextSplitter(separator=\" \", chunk_size=1024, chunk_overlap=0)\n",
        "    with open(json_file, \"r\") as f:\n",
        "        json_data = json.load(f)\n",
        "\n",
        "        for column_name, column_content in json_data.items():\n",
        "            if isinstance(column_content, str):\n",
        "                chunks = splitter.split_text(column_content)\n",
        "                for i in range(min(num_chunks, len(chunks))):\n",
        "                    chunk = chunks[i]\n",
        "                    source_chunks.append(Document(page_content=chunk, metadata={\"source\": str(Path(json_file))}))\n",
        "    return source_chunks\n",
        "\n",
        "def process_folder(folder_path, num_chunks):\n",
        "    all_chunks = []\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for dir in dirs:\n",
        "            dir_path = os.path.join(root, dir)\n",
        "            for json_file in os.listdir(dir_path):\n",
        "                json_path = os.path.join(dir_path, json_file)\n",
        "                source_chunks = get_local_chunks(json_path, num_chunks)\n",
        "                all_chunks.extend(source_chunks)\n",
        "    return all_chunks\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/Colab Notebooks/patents/AllPatentTextDocs\"  # Update with your folder path\n",
        "num_chunks = 4  # Adjust this number as per your requirement\n",
        "\n",
        "source_chunks = process_folder(folder_path, num_chunks)\n",
        "\n",
        "search_index = Chroma.from_documents(source_chunks, OpenAIEmbeddings(openai_api_key='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'))\n",
        "\n",
        "prompt_template = \"\"\"Use the context below to write a 400 word blog post about the topic below:\n",
        "    Context: {context}\n",
        "    Topic: {topic}\n",
        "    Blog post:\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"topic\"])\n",
        "\n",
        "llm = OpenAI(openai_api_key='', temperature=0)\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=PROMPT)\n",
        "\n",
        "def generate_blog_post(topic):\n",
        "    docs = search_index.similarity_search(topic, k=2)\n",
        "    inputs = [{\"context\": doc.page_content, \"topic\": topic} for doc in docs]\n",
        "    outputs = chain.apply(inputs)\n",
        "    for output in outputs:\n",
        "        print(output)\n",
        "\n",
        "generate_blog_post(\"give me the summary of abstract in the US10048858\")\n"
      ],
      "metadata": {
        "id": "bo1-ViLfFHi0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "489dfa74-c985-432d-c364-6a331db7439a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': '\\n\\nThe US10048858 patent is an apparatus that includes a touch sensitive display, a memory, and a processor. The processor is configured to cause the display to show a first image, and to automatically determine and/or create a transition for the first image and an adjacent image based on transition data comprised in or linked with the first image and the adjacent image. The transition data includes information such as an edited version of the respective image being available, the respective image being part of a sequence of images, and the respective image being part of a combination image. \\n\\nWhen the transition data shows that an edited version of the image is available, the transition comprises sliding the image into the display synchronized with the speed and direction of the swipe and blending into at least one edited version of the image as the image is fully on the display. When the transition data does not show that an edited version of the image is available, the transition comprises sliding the image into the display synchronized with the speed and direction of the swipe and blending into the adjacent image as the image is fully on the display. \\n\\nThe US10048858 patent is a useful tool for creating smooth transitions between images on a touch sensitive display. It allows for the creation of transitions that are synchronized with the'}\n",
            "{'text': '\\n\\nThe US10048858 patent is an apparatus that includes a touch sensitive display, a memory configured to store images, and a processor configured to cause the display of a first image on the touch sensitive display. The processor is also configured to automatically determine and/or create a transition for the first image and an adjacent image based on transition data comprised in or linked with the first image and the adjacent image. The transition data includes information such as an edited version of the respective image being available, the respective image being part of a sequence of images, and the respective image being part of a combination image. \\n\\nWhen the transition data shows that an edited version of the image is available, the transition comprises sliding the image into the display synchronized with the speed and direction of the swipe and blending into at least one edited version of the image as the image is fully on the display. When the transition data does not show that an edited version of the image is available, the transition comprises sliding the image into the display synchronized with the speed and direction of the swipe and blending into the adjacent image as the image is fully on the display. \\n\\nThe US10048858 patent is a useful tool for creating smooth transitions between images on a touch sensitive display. By automatically determining and'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_blog_post(topic):\n",
        "    docs = search_index.similarity_search(topic, k=2)\n",
        "    inputs = [{\"context\": doc.page_content, \"topic\": topic} for doc in docs]\n",
        "    outputs = chain.apply(inputs)\n",
        "    for output in outputs:\n",
        "        print(output)\n",
        "    for doc in docs:\n",
        "        print(doc.page_content)\n",
        "\n",
        "generate_blog_post(\"US10048858\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTbeNaC3utTz",
        "outputId": "428c329e-35d4-40bf-c6c9-d6afd0bd183e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': '\\n\\nThe US10048858 patent is an apparatus that includes a touch sensitive display, a memory configured to store images, and a processor configured to cause the display of a first image on the touch sensitive display. The processor is also configured to automatically determine and/or create a transition for the first image and an adjacent image based on transition data comprised in or linked with the first image and the adjacent image.\\n\\nThe transition data includes information such as an edited version of the respective image being available, the respective image being part of a sequence of images, and the respective image being part of a combination image. Depending on the transition data, the transition can include sliding the image into the display synchronized with the speed and direction of the swipe and blending into at least one edited version of the image as the image is fully on the display.\\n\\nThe US10048858 patent is a great tool for creating a smooth transition between images. It can be used to create a more dynamic and engaging experience for viewers. For example, if you are creating a video, you can use the US10048858 patent to create a smooth transition between images. This can help to create a more professional and visually appealing video.\\n\\nThe US10048858 patent can also be used to'}\n",
            "{'text': '\\n\\nThe US10048858 patent is an apparatus that includes a touch sensitive display, a memory configured to store images, and a processor configured to cause the display of a first image on the touch sensitive display. The processor is also configured to automatically determine and/or create a transition for the first image and an adjacent image based on transition data comprised in or linked with the first image and the adjacent image.\\n\\nThe transition data includes information such as an edited version of the respective image being available, the respective image being part of a sequence of images, and the respective image being part of a combination image. Depending on the transition data, the transition can include sliding the image into the display synchronized with the speed and direction of the swipe and blending into at least one edited version of the image as the image is fully on the display.\\n\\nThe US10048858 patent is a great tool for creating a smooth transition between images. It can be used to create a more dynamic and engaging experience for viewers. For example, if you are creating a video, you can use the US10048858 patent to create a smooth transition between images. This can help to create a more professional and visually appealing video.\\n\\nThe US10048858 patent can also be used to'}\n",
            "['What is claimed is:', '1. An apparatus, comprising:\\na touch sensitive display;\\na memory configured to store images; and\\na processor configured to cause:\\ndisplaying a first image on the touch sensitive display;\\nautomatically determining and/or creating a transition for the first image and an adjacent image based on transition data comprised in or linked with the first image and the adjacent image, wherein the transition data comprises information selected from the group of an edited version of the respective image being available, the respective image being part of a sequence of images and the respective image being part of a combination image;\\nwherein the transition comprises, when the transition data shows that an edited version of the image is available, sliding the image into the display synchronized with the speed and direction of the swipe and blending into at least one edited version of the image as the image is fully on the display; or\\nwherein the transition comprises, when transition data\n",
            "['What is claimed is:', '1. An apparatus, comprising:\\na touch sensitive display;\\na memory configured to store images; and\\na processor configured to cause:\\ndisplaying a first image on the touch sensitive display;\\nautomatically determining and/or creating a transition for the first image and an adjacent image based on transition data comprised in or linked with the first image and the adjacent image, wherein the transition data comprises information selected from the group of an edited version of the respective image being available, the respective image being part of a sequence of images and the respective image being part of a combination image;\\nwherein the transition comprises, when the transition data shows that an edited version of the image is available, sliding the image into the display synchronized with the speed and direction of the swipe and blending into at least one edited version of the image as the image is fully on the display; or\\nwherein the transition comprises, when transition data\n"
          ]
        }
      ]
    }
  ]
}