{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "####  Abstract Rogue Score\n",
        "from rouge import Rouge\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# Load the DataFrame from the Excel file\n",
        "input_file = \"abstract_summary_t5-small_Score.xlsx\"\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Create a Rouge object\n",
        "rouge = Rouge()\n",
        "\n",
        "# Define lists to store the overlap, coherence, and informativeness scores\n",
        "overlap_scores = []\n",
        "coherence_scores = []\n",
        "informativeness_scores = []\n",
        "\n",
        "# Set device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Iterate over the rows in the DataFrame with a progress bar\n",
        "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
        "    # Get the original text and the generated summary\n",
        "    abstract = row['Abstract']\n",
        "\n",
        "    # Clean the abstract and claims text\n",
        "    abstract = re.sub(r'[^\\x00-\\x7F]+', '', str(abstract))\n",
        "\n",
        "    # Combine the cleaned abstract and claims\n",
        "    original_text = abstract\n",
        "    generated_summary = str(row['Abstract_Summary'])\n",
        "\n",
        "    # Calculate ROUGE scores\n",
        "    scores = rouge.get_scores(generated_summary, original_text)\n",
        "\n",
        "    # Extract the relevant scores\n",
        "    overlap_score = scores[0]['rouge-1']['f']\n",
        "    coherence_score = scores[0]['rouge-2']['f']\n",
        "    informativeness_score = scores[0]['rouge-l']['f']\n",
        "\n",
        "    # Append the scores to the respective lists\n",
        "    overlap_scores.append(overlap_score)\n",
        "    coherence_scores.append(coherence_score)\n",
        "    informativeness_scores.append(informativeness_score)\n",
        "\n",
        "# Add the scores to the DataFrame\n",
        "df[\"Overlap_Score['rouge-1']\"] = overlap_scores\n",
        "df[\"Coherence_Score['rouge-2']\"] = coherence_scores\n",
        "df[\"Informativeness_Score['rouge-l']\"] = informativeness_scores\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "output_file = \"Summary_Scores_hupd_Rouge_score_t5_small_total.xlsx\"\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "# Print the average scores\n",
        "print(\"Average Overlap Score:\", sum(overlap_scores) / len(overlap_scores))\n",
        "print(\"Average Coherence Score:\", sum(coherence_scores) / len(coherence_scores))\n",
        "print(\"Average Informativeness Score:\", sum(informativeness_scores) / len(informativeness_scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xJP6KUpcvTr",
        "outputId": "7f794eee-d96e-4ac9-a4b7-8265da171dcd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating Scores: 100%|██████████| 1630/1630 [00:09<00:00, 180.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Overlap Score: 0.7022384785848844\n",
            "Average Coherence Score: 0.6146253621342672\n",
            "Average Informativeness Score: 0.6997961547500037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Rouge"
      ],
      "metadata": {
        "id": "UWS-G58wNw90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Claims Rogue Score\n",
        "from rouge import Rouge\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# Load the DataFrame from the Excel file\n",
        "input_file = \"claim_summary_t5-small_score.xlsx\"\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Create a Rouge object\n",
        "rouge = Rouge()\n",
        "\n",
        "# Define lists to store the overlap, coherence, and informativeness scores\n",
        "overlap_scores = []\n",
        "coherence_scores = []\n",
        "informativeness_scores = []\n",
        "\n",
        "# Set device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Iterate over the rows in the DataFrame with a progress bar\n",
        "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
        "    # Get the original text and the generated summary\n",
        "    claim = row['Claims']\n",
        "\n",
        "    # Clean the abstract and claims text\n",
        "    claims = re.sub(r'[^\\x00-\\x7F]+', '', str(claim))\n",
        "\n",
        "    # Combine the cleaned abstract and claims\n",
        "    original_text = claims\n",
        "    generated_summary = str(row['claim_Summary'])\n",
        "\n",
        "    # Calculate ROUGE scores\n",
        "    scores = rouge.get_scores(generated_summary, original_text)\n",
        "\n",
        "    # Extract the relevant scores\n",
        "    overlap_score = scores[0]['rouge-1']['f']\n",
        "    coherence_score = scores[0]['rouge-2']['f']\n",
        "    informativeness_score = scores[0]['rouge-l']['f']\n",
        "\n",
        "    # Append the scores to the respective lists\n",
        "    overlap_scores.append(overlap_score)\n",
        "    coherence_scores.append(coherence_score)\n",
        "    informativeness_scores.append(informativeness_score)\n",
        "\n",
        "# Add the scores to the DataFrame\n",
        "df[\"Overlap_Score['rouge-1']\"] = overlap_scores\n",
        "df[\"Coherence_Score['rouge-2']\"] = coherence_scores\n",
        "df[\"Informativeness_Score['rouge-l']\"] = informativeness_scores\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "output_file = \"Summary_Scores_hupd_Rouge_score_t5_small_Claims_total.xlsx\"\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "# Print the average scores\n",
        "print(\"Average Overlap Score:\", sum(overlap_scores) / len(overlap_scores))\n",
        "print(\"Average Coherence Score:\", sum(coherence_scores) / len(coherence_scores))\n",
        "print(\"Average Informativeness Score:\", sum(informativeness_scores) / len(informativeness_scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbfKUJdzWBEe",
        "outputId": "bdb4543a-892d-44b5-d6aa-d9b118b354f5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating Scores: 100%|██████████| 1630/1630 [01:37<00:00, 16.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Overlap Score: 0.31694796876483294\n",
            "Average Coherence Score: 0.19459843339511917\n",
            "Average Informativeness Score: 0.315735633815753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Combined summary Rogue Score\n",
        "from rouge import Rouge\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# Load the DataFrame from the Excel file\n",
        "input_file = \"Combined_Google_patent_Summary_t5_small.xlsx\"\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Create a Rouge object\n",
        "rouge = Rouge()\n",
        "\n",
        "# Define lists to store the overlap, coherence, and informativeness scores\n",
        "indices = []\n",
        "overlap_scores = []\n",
        "coherence_scores = []\n",
        "informativeness_scores = []\n",
        "\n",
        "# Set device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ... the rest of your code\n",
        "\n",
        "# Iterate over the rows in the DataFrame with a progress bar\n",
        "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
        "    # Get the original text and the generated summary\n",
        "    abstract = row['Abstract_Summary']\n",
        "    claim = row['claim_Summary']\n",
        "\n",
        "    # Clean the abstract and claims text\n",
        "    abstract = re.sub(r'[^\\x00-\\x7F]+', '', str(abstract))\n",
        "    claims = re.sub(r'[^\\x00-\\x7F]+', '', str(claim))\n",
        "\n",
        "    # Combine the cleaned abstract and claims\n",
        "    original_text = abstract + ' ' + claims\n",
        "    generated_summary = str(row['Combined_Summary'])\n",
        "\n",
        "    # Skip the iteration if generated_summary is empty\n",
        "    if not generated_summary.strip():\n",
        "        print(f\"Skipping row {index} because generated_summary is empty\")\n",
        "        continue\n",
        "\n",
        "    # Calculate ROUGE scores\n",
        "    try:\n",
        "        scores = rouge.get_scores(generated_summary, original_text)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error in row {index}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Extract the relevant scores\n",
        "    overlap_score = scores[0]['rouge-1']['f']\n",
        "    coherence_score = scores[0]['rouge-2']['f']\n",
        "    informativeness_score = scores[0]['rouge-l']['f']\n",
        "\n",
        "    # Append the index and scores to the respective lists\n",
        "    indices.append(index)\n",
        "    overlap_scores.append(overlap_score)\n",
        "    coherence_scores.append(coherence_score)\n",
        "    informativeness_scores.append(informativeness_score)\n",
        "\n",
        "\n",
        "# Assign scores to the DataFrame for the specific indices\n",
        "df.loc[indices, \"Overlap_Score['rouge-1']\"] = overlap_scores\n",
        "df.loc[indices, \"Coherence_Score['rouge-2']\"] = coherence_scores\n",
        "df.loc[indices, \"Informativeness_Score['rouge-l']\"] = informativeness_scores\n",
        "\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "output_file = \"Summary_Scores_hupd_Rouge_score_t5_small_Combined_Summary_total.xlsx\"\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "# Print the average scores\n",
        "print(\"Average Overlap Score:\", sum(overlap_scores) / len(overlap_scores))\n",
        "print(\"Average Coherence Score:\", sum(coherence_scores) / len(coherence_scores))\n",
        "print(\"Average Informativeness Score:\", sum(informativeness_scores) / len(informativeness_scores))\n"
      ],
      "metadata": {
        "id": "25H6XPkxdMjB",
        "outputId": "cd38de83-bba2-4dfa-cf3e-6a076a468626",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating Scores:  20%|█▉        | 323/1630 [00:01<00:07, 165.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in row 304: Hypothesis is empty.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating Scores: 100%|██████████| 1630/1630 [00:12<00:00, 126.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Overlap Score: 0.7367705380119189\n",
            "Average Coherence Score: 0.6575756066337907\n",
            "Average Informativeness Score: 0.736232712910879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lSrDAOnUmQi",
        "outputId": "3dcb8de5-d53c-4c38-e4f9-17f9c333479f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YZVvfOtSk1Tq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}