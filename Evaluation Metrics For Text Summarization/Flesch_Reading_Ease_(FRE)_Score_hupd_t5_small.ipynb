{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl textstat\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eukJxn9oSY7t",
        "outputId": "caf3d683-fea1-4ace-cc70-a519e22b3213"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.0.10)\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/105.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.14.0 textstat-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Flesch Reading Ease (FRE) score ranges from 0 to 100. Higher scores indicate material that is easier to read; lower numbers mark passages that are more complex and harder to read. Here's a general interpretation of the Flesch Reading Ease scores:\n",
        "\n",
        "90-100: Very Easy to read, easily understood by an average 11-year-old student.\n",
        "80-89: Easy to read.\n",
        "70-79: Fairly easy to read.\n",
        "60-69: Standard, Plain English, easily understood by 13- to 15-year-old students.\n",
        "50-59: Fairly difficult to read.\n",
        "30-49: Difficult to read.\n",
        "0-29: Very difficult to read, best understood by university graduates.\n",
        "\n",
        "With an average FRE score of approximately 17, the text is considered very difficult to read and is best understood by university graduates. Such a score might be common in scientific papers, legal documents, or technical documents which use specialized terminology and complex sentence structures."
      ],
      "metadata": {
        "id": "OUBBolMMUVkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#abstract FRE score\n",
        "\n",
        "import pandas as pd\n",
        "import textstat\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# Load the DataFrame from the Excel file\n",
        "input_file = \"abstract_summary_t5_small.xlsx\"\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Define list to store the Flesch Reading Ease scores\n",
        "fre_scores = []\n",
        "\n",
        "# Iterate over the rows in the DataFrame with a progress bar\n",
        "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
        "    # Get the generated summary\n",
        "    abstract_summary = str(row['Abstract_Summary'])\n",
        "\n",
        "    # Clean the abstract summary text\n",
        "    abstract_summary = re.sub(r'[^\\x00-\\x7F]+', '', abstract_summary)\n",
        "\n",
        "    # Calculate Flesch Reading Ease score\n",
        "    fre_score = textstat.flesch_reading_ease(abstract_summary)\n",
        "\n",
        "    # Append the score to the list\n",
        "    fre_scores.append(fre_score)\n",
        "\n",
        "# Add the FRE scores to the DataFrame\n",
        "df['FRE_Score'] = fre_scores\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "output_file = \"Summary_FRE_Scores_t5_small_abstract_total.xlsx\"\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "# Print the average FRE score\n",
        "print(\"\\nAverage Flesch Reading Ease Score for Abstract Summary:\", sum(fre_scores) / len(fre_scores))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRqVGU4sOS6T",
        "outputId": "15c7b072-5c12-4059-b8ef-626df6e7be5e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating Scores: 100%|██████████| 1630/1630 [00:01<00:00, 1439.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Flesch Reading Ease Score for Abstract Summary: 27.135263803680953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#claims FRE score\n",
        "\n",
        "import pandas as pd\n",
        "import textstat\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# Load the DataFrame from the Excel file\n",
        "input_file = \"claim_summary_t5_small.xlsx\"\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Define list to store the Flesch Reading Ease scores\n",
        "fre_scores = []\n",
        "\n",
        "# Iterate over the rows in the DataFrame with a progress bar\n",
        "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
        "    # Get the generated summary\n",
        "    claim_Summary = str(row['claim_Summary'])\n",
        "\n",
        "    # Clean the abstract summary text\n",
        "    claim_Summary = re.sub(r'[^\\x00-\\x7F]+', '', claim_Summary)\n",
        "\n",
        "    # Calculate Flesch Reading Ease score\n",
        "    fre_score = textstat.flesch_reading_ease(claim_Summary)\n",
        "\n",
        "    # Append the score to the list\n",
        "    fre_scores.append(fre_score)\n",
        "\n",
        "# Add the FRE scores to the DataFrame\n",
        "df['FRE_Score'] = fre_scores\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "output_file = \"Summary_FRE_Scores_t5_small_claims_total.xlsx\"\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "# Print the average FRE score\n",
        "print(\"\\nAverage Flesch Reading Ease Score:\", sum(fre_scores) / len(fre_scores))\n"
      ],
      "metadata": {
        "id": "YZVvfOtSk1Tq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7afabc9-807e-47b1-f604-3983343e3479"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating Scores: 100%|██████████| 1630/1630 [00:00<00:00, 3545.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Flesch Reading Ease Score: 37.329159509202434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#combined FRE score\n",
        "\n",
        "import pandas as pd\n",
        "import textstat\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# Load the DataFrame from the Excel file\n",
        "input_file = \"Combined_Google_patent_Summary_t5_small_score.xlsx\"\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Define list to store the Flesch Reading Ease scores\n",
        "fre_scores = []\n",
        "\n",
        "# Iterate over the rows in the DataFrame with a progress bar\n",
        "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
        "    # Get the generated summary\n",
        "    Combined_Summary = str(row['Combined_Summary'])\n",
        "\n",
        "    # Clean the abstract summary text\n",
        "    Combined_Summary = re.sub(r'[^\\x00-\\x7F]+', '', Combined_Summary)\n",
        "\n",
        "    # Calculate Flesch Reading Ease score\n",
        "    fre_score = textstat.flesch_reading_ease(Combined_Summary)\n",
        "\n",
        "    # Append the score to the list\n",
        "    fre_scores.append(fre_score)\n",
        "\n",
        "# Add the FRE scores to the DataFrame\n",
        "df['FRE_Score'] = fre_scores\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "output_file = \"Summary_FRE_Scores_t5_small_combined_total.xlsx\"\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "# Print the average FRE score\n",
        "print(\"\\nAverage Flesch Reading Ease Score:\", sum(fre_scores) / len(fre_scores))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAiY9w0wMBgn",
        "outputId": "0b239e04-7484-4e65-cabb-a40679f75520"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating Scores: 100%|██████████| 1630/1630 [00:00<00:00, 4310.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Flesch Reading Ease Score: 36.2081472392638\n"
          ]
        }
      ]
    }
  ]
}