{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdGMVf1IdeAe",
        "outputId": "f771721c-4bb9-429e-b564-d00c7420f130"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating Scores:  11%|█         | 181/1630 [00:00<00:00, 1806.91it/s]/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "Calculating Scores:  22%|██▏       | 362/1630 [00:00<00:00, 1747.86it/s]/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "Calculating Scores: 100%|██████████| 1630/1630 [00:01<00:00, 1382.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average BLEU Score: 0.35321952350168034\n"
          ]
        }
      ],
      "source": [
        "#abstract bleu score\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# Load the DataFrame from the Excel file\n",
        "input_file = \"abstract_summary_t5_small.xlsx\"\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Define lists to store the scores\n",
        "bleu_scores = []\n",
        "fluency_scores = []\n",
        "accuracy_scores = []\n",
        "readability_scores = []\n",
        "relevance_scores = []\n",
        "novelty_scores = []\n",
        "style_scores = []\n",
        "\n",
        "# Iterate over the rows in the DataFrame with a progress bar\n",
        "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
        "    # Get the original text and the generated summary\n",
        "    abstract = row['Abstract']\n",
        "    #claims = row['Claims']\n",
        "\n",
        "    # Clean the abstract and claims text\n",
        "    abstract = re.sub(r'[^\\x00-\\x7F]+', '', str(abstract))\n",
        "    #claims = re.sub(r'[^\\x00-\\x7F]+', '', str(claims))\n",
        "\n",
        "    # Combine the cleaned abstract and claims\n",
        "    original_text = abstract\n",
        "    generated_summary = str(row['Abstract_Summary'])\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    reference = [original_text.split()]\n",
        "    candidate = generated_summary.split()\n",
        "    bleu_score = sentence_bleu(reference, candidate)\n",
        "\n",
        "    # Append the BLEU score\n",
        "    bleu_scores.append(bleu_score)\n",
        "\n",
        "# Add the scores to the DataFrame\n",
        "df['BLEU_Score'] = bleu_scores\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "output_file = \"Summary_Scores_t5_small_Bleu_score_abstract_total.xlsx\"\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "# Print the average scores\n",
        "print(\"\\nAverage BLEU Score:\", sum(bleu_scores) / len(bleu_scores))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#claims bleu score\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# Load the DataFrame from the Excel file\n",
        "input_file = \"claim_summary_t5_small.xlsx\"\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Define lists to store the scores\n",
        "bleu_scores = []\n",
        "fluency_scores = []\n",
        "accuracy_scores = []\n",
        "readability_scores = []\n",
        "relevance_scores = []\n",
        "novelty_scores = []\n",
        "style_scores = []\n",
        "\n",
        "# Iterate over the rows in the DataFrame with a progress bar\n",
        "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
        "    # Get the original text and the generated summary\n",
        "    #abstract = row['Abstract']\n",
        "    claims = row['Claims']\n",
        "\n",
        "    # Clean the abstract and claims text\n",
        "    #abstract = re.sub(r'[^\\x00-\\x7F]+', '', str(abstract))\n",
        "    claims = re.sub(r'[^\\x00-\\x7F]+', '', str(claims))\n",
        "\n",
        "    # Combine the cleaned abstract and claims\n",
        "    original_text = claims\n",
        "    generated_summary = str(row['claim_Summary'])\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    reference = [original_text.split()]\n",
        "    candidate = generated_summary.split()\n",
        "    bleu_score = sentence_bleu(reference, candidate)\n",
        "\n",
        "    # Append the BLEU score\n",
        "    bleu_scores.append(bleu_score)\n",
        "\n",
        "# Add the scores to the DataFrame\n",
        "df['BLEU_Score'] = bleu_scores\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "output_file = \"Summary_Scores_t5_small_Bleu_score_claims_total.xlsx\"\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "# Print the average scores\n",
        "print(\"\\nAverage BLEU Score:\", sum(bleu_scores) / len(bleu_scores))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hc0F5YaRn-Un",
        "outputId": "ed08a275-d4fc-42a2-9ad3-be070f629060"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating Scores: 100%|██████████| 1630/1630 [00:03<00:00, 468.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average BLEU Score: 0.001245566846491637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#combined bleu score\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# Load the DataFrame from the Excel file\n",
        "input_file = \"Combined_Google_patent_Summary_t5_small_score.xlsx\"\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Define lists to store the scores\n",
        "bleu_scores = []\n",
        "fluency_scores = []\n",
        "accuracy_scores = []\n",
        "readability_scores = []\n",
        "relevance_scores = []\n",
        "novelty_scores = []\n",
        "style_scores = []\n",
        "\n",
        "# Iterate over the rows in the DataFrame with a progress bar\n",
        "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
        "    # Get the original text and the generated summary\n",
        "    abstract = row['Abstract_Summary']\n",
        "    claims = row['claim_Summary']\n",
        "\n",
        "    # Clean the abstract and claims text\n",
        "    abstract = re.sub(r'[^\\x00-\\x7F]+', '', str(abstract))\n",
        "    claims = re.sub(r'[^\\x00-\\x7F]+', '', str(claims))\n",
        "\n",
        "    # Combine the cleaned abstract and claims\n",
        "    original_text = abstract+' '+claims\n",
        "    generated_summary = str(row['Combined_Summary'])\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    reference = [original_text.split()]\n",
        "    candidate = generated_summary.split()\n",
        "    bleu_score = sentence_bleu(reference, candidate)\n",
        "\n",
        "    # Append the BLEU score\n",
        "    bleu_scores.append(bleu_score)\n",
        "\n",
        "# Add the scores to the DataFrame\n",
        "df['BLEU_Score'] = bleu_scores\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "output_file = \"Summary_Scores_t5_small_Bleu_score_combined_total.xlsx\"\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "# Print the average scores\n",
        "print(\"\\nAverage BLEU Score:\", sum(bleu_scores) / len(bleu_scores))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdL6cwc-oZaq",
        "outputId": "22607f55-9898-4581-b2f3-e407caee26c0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating Scores:  12%|█▏        | 194/1630 [00:00<00:00, 1935.81it/s]/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "Calculating Scores: 100%|██████████| 1630/1630 [00:01<00:00, 1385.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average BLEU Score: 0.38927261609484437\n"
          ]
        }
      ]
    }
  ]
}