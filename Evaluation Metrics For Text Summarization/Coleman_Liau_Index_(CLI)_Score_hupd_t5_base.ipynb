{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The Coleman-Liau Index (CLI) is a readability test for English text, which approximates the US grade level thought necessary to understand the text. The formula for calculating the CLI is:\n",
        "\n",
        "CLI = 0.0588 * L - 0.296 * S - 15.8\n",
        "\n",
        "where:\n",
        "\n",
        "L is the average number of letters per 100 words, and\n",
        "S is the average number of sentences per 100 words."
      ],
      "metadata": {
        "id": "C7hzZEdimRDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Coleman-Liau Index (CLI) is designed to approximate the U.S. grade level needed to understand a text. The score usually ranges from around 0 to 16, where 0 represents the reading level of a kindergartner and 16 corresponds to a college graduate's reading level.\n",
        "\n",
        "Here is a rough interpretation of the scores:\n",
        "\n",
        "0 - Kindergarten\n",
        "\n",
        "1-6 - Elementary School (1st to 6th grade)\n",
        "\n",
        "7-8 - Middle School (7th to 8th grade)\n",
        "\n",
        "9-12 - High School (9th to 12th grade)\n",
        "\n",
        "13-16 - College level and above\n",
        "\n",
        "With an average CLI score of approximately 13.9, the text is estimated to be at the reading level of a college student. This means the text is fairly complex and might not be easily understood by individuals with lower education levels.\n",
        "\n",
        "If the intended audience for your abstract summaries is scholars, researchers, or people with higher education, this score could be appropriate. However, if you're targeting a general audience, you might want to simplify the text to make it more accessible."
      ],
      "metadata": {
        "id": "_Fdxqe0qnBKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#abstract code\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# Load the DataFrame from the Excel file\n",
        "input_file = \"Abstract_Summary_t5_base_file.xlsx\"\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Define lists to store the CLI scores\n",
        "cli_scores = []\n",
        "\n",
        "# Iterate over the rows in the DataFrame with a progress bar\n",
        "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating CLI Scores\"):\n",
        "    # Get the generated summary\n",
        "    generated_summary = str(row['Abstract_Summary_t5_base'])\n",
        "\n",
        "    # Clean the generated_summary\n",
        "    generated_summary = re.sub(r'[^\\x00-\\x7F]+', '', generated_summary)\n",
        "\n",
        "    # Count the number of letters and sentences in generated_summary\n",
        "    num_letters = sum(c.isalpha() for c in generated_summary)\n",
        "    num_sentences = generated_summary.count('.') + generated_summary.count('!') + generated_summary.count('?')\n",
        "    num_words = len(generated_summary.split())\n",
        "\n",
        "    # Calculate average letters and sentences per 100 words\n",
        "    L = (num_letters / num_words) * 100 if num_words > 0 else 0\n",
        "    S = (num_sentences / num_words) * 100 if num_words > 0 else 0\n",
        "\n",
        "    # Calculate the Coleman-Liau Index\n",
        "    cli_score = 0.0588 * L - 0.296 * S - 15.8\n",
        "\n",
        "    # Append the CLI score to the list\n",
        "    cli_scores.append(cli_score)\n",
        "\n",
        "# Add the CLI scores to the DataFrame\n",
        "df['CLI_Score'] = cli_scores\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "output_file = \"CLI_Scores_t5_base_abstract_total.xlsx\"\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "# Print the average CLI score\n",
        "print(\"\\nAverage CLI Score:\", sum(cli_scores) / len(cli_scores) if len(cli_scores) > 0 else 0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tT9pKbU8mTlV",
        "outputId": "55d3bdcf-9584-4957-8090-247f52d1f7e5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating CLI Scores: 100%|██████████| 1630/1630 [00:00<00:00, 6682.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average CLI Score: 13.730044964442536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#claims code\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# Load the DataFrame from the Excel file\n",
        "input_file = \"Claims_Summary_t5_base_file.xlsx\"\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Define lists to store the CLI scores\n",
        "cli_scores = []\n",
        "\n",
        "# Iterate over the rows in the DataFrame with a progress bar\n",
        "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating CLI Scores\"):\n",
        "    # Get the generated summary\n",
        "    claim_Summary = str(row['Claims_Summary_t5_base'])\n",
        "\n",
        "    # Clean the generated_summary\n",
        "    generated_summary = re.sub(r'[^\\x00-\\x7F]+', '', claim_Summary)\n",
        "\n",
        "    # Count the number of letters and sentences in generated_summary\n",
        "    num_letters = sum(c.isalpha() for c in generated_summary)\n",
        "    num_sentences = generated_summary.count('.') + generated_summary.count('!') + generated_summary.count('?')\n",
        "    num_words = len(generated_summary.split())\n",
        "\n",
        "    # Calculate average letters and sentences per 100 words\n",
        "    L = (num_letters / num_words) * 100 if num_words > 0 else 0\n",
        "    S = (num_sentences / num_words) * 100 if num_words > 0 else 0\n",
        "\n",
        "    # Calculate the Coleman-Liau Index\n",
        "    cli_score = 0.0588 * L - 0.296 * S - 15.8\n",
        "\n",
        "    # Append the CLI score to the list\n",
        "    cli_scores.append(cli_score)\n",
        "\n",
        "# Add the CLI scores to the DataFrame\n",
        "df['CLI_Score'] = cli_scores\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "output_file = \"CLI_Scores_t5_base_claims_total.xlsx\"\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "# Print the average CLI score\n",
        "print(\"\\nAverage CLI Score:\", sum(cli_scores) / len(cli_scores) if len(cli_scores) > 0 else 0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mC_k7x7nFxg",
        "outputId": "425dad28-dc3f-4e8e-93b6-038454f658c4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating CLI Scores: 100%|██████████| 1630/1630 [00:00<00:00, 6198.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average CLI Score: 13.371598078429653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#combined code\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# Load the DataFrame from the Excel file\n",
        "input_file = \"Combined_Google_patent_Summary_t5_base_file.xlsx\"\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Define lists to store the CLI scores\n",
        "cli_scores = []\n",
        "\n",
        "# Iterate over the rows in the DataFrame with a progress bar\n",
        "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating CLI Scores\"):\n",
        "    # Get the generated summary\n",
        "    Combined_Summary = str(row['Combined_Summary'])\n",
        "\n",
        "    # Clean the generated_summary\n",
        "    generated_summary = re.sub(r'[^\\x00-\\x7F]+', '', Combined_Summary)\n",
        "\n",
        "    # Count the number of letters and sentences in generated_summary\n",
        "    num_letters = sum(c.isalpha() for c in generated_summary)\n",
        "    num_sentences = generated_summary.count('.') + generated_summary.count('!') + generated_summary.count('?')\n",
        "    num_words = len(generated_summary.split())\n",
        "\n",
        "    # Calculate average letters and sentences per 100 words\n",
        "    L = (num_letters / num_words) * 100 if num_words > 0 else 0\n",
        "    S = (num_sentences / num_words) * 100 if num_words > 0 else 0\n",
        "\n",
        "    # Calculate the Coleman-Liau Index\n",
        "    cli_score = 0.0588 * L - 0.296 * S - 15.8\n",
        "\n",
        "    # Append the CLI score to the list\n",
        "    cli_scores.append(cli_score)\n",
        "\n",
        "# Add the CLI scores to the DataFrame\n",
        "df['CLI_Score'] = cli_scores\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "output_file = \"CLI_Scores_t5_base_combined_total.xlsx\"\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "# Print the average CLI score\n",
        "print(\"\\nAverage CLI Score:\", sum(cli_scores) / len(cli_scores) if len(cli_scores) > 0 else 0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3IEw8Xxnkqp",
        "outputId": "90cda4d6-0744-45d4-8238-6f568d6b8390"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating CLI Scores: 100%|██████████| 1630/1630 [00:00<00:00, 10433.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average CLI Score: 13.595977196087055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ARRlxrZpmyKb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}