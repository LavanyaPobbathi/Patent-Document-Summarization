{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The BLEU (Bilingual Evaluation Understudy) score is a metric used to evaluate the quality of machine-generated translations against one or more reference translations. It measures the similarity between the generated translation and the reference translations based on n-gram matches.\n",
        "\n",
        "The BLEU score ranges between 0 and 1, with 1 being a perfect match. It is calculated by comparing the n-grams (contiguous sequences of words) in the generated translation to the n-grams in the reference translations. BLEU considers precision, which is the percentage of n-grams in the generated translation that also appear in the reference translations, and brevity penalty, which penalizes shorter translations.\n",
        "\n",
        "BLEU is commonly used in machine translation tasks to assess the quality of generated translations and compare different translation systems. It provides a quantitative measure of how well the generated translation aligns with the reference translations in terms of word overlap."
      ],
      "metadata": {
        "id": "Dusep9ryPhFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying to compare BLEU scores across different corpora and languages is strongly discouraged. Even comparing BLEU scores for the same corpus but with different numbers of reference translations can be highly misleading.\n",
        "\n",
        "However, as a rough guideline, the following interpretation of BLEU scores (expressed as percentages rather than decimals) might be helpful.\n",
        "\n",
        "BLEU Score\tInterpretation\n",
        "\n",
        "< 10\tAlmost useless\n",
        "\n",
        "10 - 19\tHard to get the gist\n",
        "\n",
        "20 - 29\tThe gist is clear, but has significant grammatical errors\n",
        "\n",
        "30 - 40\tUnderstandable to good translations\n",
        "\n",
        "40 - 50\tHigh quality translations\n",
        "\n",
        "50 - 60\tVery high quality, adequate, and fluent translations\n",
        "\n",
        "greater than 60\tQuality often better than human\n",
        "\n",
        "****NOTE*****\n",
        "Bleu Scores are between 0 and 1. A score of 0.6 or 0.7 is considered the best you can achieve. Even two humans would likely come up with different sentence variants for a problem, and would rarely achieve a perfect match."
      ],
      "metadata": {
        "id": "uSRMZXhUFNV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu"
      ],
      "metadata": {
        "id": "rzjs5uRsPiI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('pegasus-x-large-booksum-1.xlsx', nrows=1630)\n",
        "\n",
        "columns_to_keep = ['Filename', 'Abstract', 'Claims', 'Summary'] # Add other column names here\n",
        "df_subset = df[columns_to_keep]\n",
        "abstract_and_claims = df['Abstract'] + ' ' + df['Claims']\n",
        "\n",
        "summary = df['Summary']\n",
        "\n",
        "bleu_scores_df = pd.DataFrame()\n",
        "\n",
        "for i in range(len(abstract_and_claims)):\n",
        "  input_content = abstract_and_claims[i]\n",
        "  reference_summary = summary[i]\n",
        "  # Split input content and reference summary into segments (e.g., paragraphs)\n",
        "  input_segments = input_content.split(\"\\n\")\n",
        "  reference_segments = reference_summary.split(\"\\n\")\n",
        "\n",
        "  # Initialize the scores for this row\n",
        "  row_scores = {}\n",
        "\n",
        "  for input_seg, reference_seg in zip(input_segments, reference_segments):\n",
        "      # Calculate BLEU score\n",
        "      bleu_score = sentence_bleu([reference_seg], input_seg)\n",
        "\n",
        "      # Add BLEU score to row_scores\n",
        "      row_scores.setdefault('bleu', []).append(bleu_score)\n",
        "\n",
        "  # Calculate the average scores for this row\n",
        "  num_segments = len(input_segments)\n",
        "  row_scores_avg = {'bleu': sum(row_scores['bleu']) / num_segments}\n",
        "\n",
        "  # Convert the row_scores_avg to a DataFrame and append to bleu_scores_df\n",
        "  row_scores_df = pd.DataFrame(row_scores_avg, index=[i])\n",
        "  bleu_scores_df = bleu_scores_df.append(row_scores_df, ignore_index=True)\n",
        "\n",
        "merged_df = pd.concat([df_subset, bleu_scores_df], axis=1)\n",
        "\n",
        "merged_df.to_csv('bleu_scores_Final.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HnYMQlRtP5iK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}