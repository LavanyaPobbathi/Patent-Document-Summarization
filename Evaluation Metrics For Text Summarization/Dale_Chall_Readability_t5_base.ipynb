{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "DCRS is the Dale-Chall Readability Score, PDW is the Percentage of Difficult Words, and ASL is the Average Sentence Length.\n",
        "\n",
        "The resulting score can be interpreted as follows:\n",
        "\n",
        "4.9 or below: Easily understandable by an average 4th-grade student or lower.\n",
        "\n",
        "5.0–5.9: Easily understandable by an average 5th or 6th-grade student.\n",
        "\n",
        "6.0–6.9: Easily understandable by an average 7th or 8th-grade student.\n",
        "\n",
        "7.0–7.9: Easily understandable by an average 9th or 10th-grade student.\n",
        "\n",
        "8.0–8.9: Easily understandable by an average 11th or 12th-grade student.\n",
        "\n",
        "9.0–9.9: Easily understandable by an average college student.\n",
        "\n",
        "10.0 or above: Only easily understandable by graduates or individuals with a higher level of education.\n"
      ],
      "metadata": {
        "id": "wJtKmfW8rJgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHxTOw7_sSWu",
        "outputId": "6d39c377-237c-4c24-d71f-8614a9c59f3c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/105.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.14.0 textstat-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import textstat\n",
        "import re\n",
        "\n",
        "# Load the DataFrame from the Excel file\n",
        "input_file = \"Abstract_Summary_t5_base_file.xlsx\"\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Define a list to store the Dale-Chall Readability scores\n",
        "dcr_scores = []\n",
        "\n",
        "# Iterate over the rows in the DataFrame with a progress bar\n",
        "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
        "    # Get the abstract summary\n",
        "    abstract_summary = str(row['Abstract_Summary_t5_base'])\n",
        "\n",
        "    # Clean the abstract summary\n",
        "    abstract_summary = re.sub(r'[^\\x00-\\x7F]+', '', abstract_summary)\n",
        "\n",
        "    # Calculate Dale-Chall Readability score\n",
        "    dcr_score = textstat.dale_chall_readability_score(abstract_summary)\n",
        "\n",
        "    # Append the score to the list\n",
        "    dcr_scores.append(dcr_score)\n",
        "\n",
        "# Add the scores to the DataFrame\n",
        "df['Dale_Chall_Readability_Score'] = dcr_scores\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "output_file = \"Dale_Chall_Readability_Scores_t5_base_abstract_total.xlsx\"\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "# Print the average Dale-Chall Readability Score\n",
        "print(\"\\nAverage Dale-Chall Readability Score:\", sum(dcr_scores) / len(dcr_scores))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMDHH2uvrJ4B",
        "outputId": "1c9b9215-30f9-497b-be73-ae0b6349ecf4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating Scores: 100%|██████████| 1630/1630 [00:01<00:00, 1385.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Dale-Chall Readability Score: 10.5803926380368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import textstat\n",
        "import re\n",
        "\n",
        "# Load the DataFrame from the Excel file\n",
        "input_file = \"Claims_Summary_t5_base_file.xlsx\"\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Define a list to store the Dale-Chall Readability scores\n",
        "dcr_scores = []\n",
        "\n",
        "# Iterate over the rows in the DataFrame with a progress bar\n",
        "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
        "    # Get the abstract summary\n",
        "    claim_Summary = str(row['Claims_Summary_t5_base'])\n",
        "\n",
        "    # Clean the abstract summary\n",
        "    claim_Summary = re.sub(r'[^\\x00-\\x7F]+', '', claim_Summary)\n",
        "\n",
        "    # Calculate Dale-Chall Readability score\n",
        "    dcr_score = textstat.dale_chall_readability_score(claim_Summary)\n",
        "\n",
        "    # Append the score to the list\n",
        "    dcr_scores.append(dcr_score)\n",
        "\n",
        "# Add the scores to the DataFrame\n",
        "df['Dale_Chall_Readability_Score'] = dcr_scores\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "output_file = \"Dale_Chall_Readability_Scores_t5_base_claims_total.xlsx\"\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "# Print the average Dale-Chall Readability Score\n",
        "print(\"\\nAverage Dale-Chall Readability Score:\", sum(dcr_scores) / len(dcr_scores))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxkqVqD9sdyx",
        "outputId": "13ba5794-020f-47e6-82f8-17901f7a651e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating Scores: 100%|██████████| 1630/1630 [00:00<00:00, 3430.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Dale-Chall Readability Score: 9.873447852760737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import textstat\n",
        "import re\n",
        "\n",
        "# Load the DataFrame from the Excel file\n",
        "input_file = \"Combined_Google_patent_Summary_t5_base_file.xlsx\"\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Define a list to store the Dale-Chall Readability scores\n",
        "dcr_scores = []\n",
        "\n",
        "# Iterate over the rows in the DataFrame with a progress bar\n",
        "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating Scores\"):\n",
        "    # Get the abstract summary\n",
        "    Combined_Summary = str(row['Combined_Summary'])\n",
        "\n",
        "    # Clean the abstract summary\n",
        "    Combined_Summary = re.sub(r'[^\\x00-\\x7F]+', '', Combined_Summary)\n",
        "\n",
        "    # Calculate Dale-Chall Readability score\n",
        "    dcr_score = textstat.dale_chall_readability_score(Combined_Summary)\n",
        "\n",
        "    # Append the score to the list\n",
        "    dcr_scores.append(dcr_score)\n",
        "\n",
        "# Add the scores to the DataFrame\n",
        "df['Dale_Chall_Readability_Score'] = dcr_scores\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "output_file = \"Dale_Chall_Readability_Scores_t5_base_combined_total.xlsx\"\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "# Print the average Dale-Chall Readability Score\n",
        "print(\"\\nAverage Dale-Chall Readability Score:\", sum(dcr_scores) / len(dcr_scores))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO1maswrsvCF",
        "outputId": "de0fc9e3-ca94-4b3e-bef1-5343eddbd761"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating Scores: 100%|██████████| 1630/1630 [00:00<00:00, 2737.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Dale-Chall Readability Score: 10.660993865030667\n"
          ]
        }
      ]
    }
  ]
}